{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "sbpngF0a5twG"
      },
      "outputs": [],
      "source": [
        "# @title Autor: Alan Echer\n",
        "# O algoritmo abaixo foi criado baseado nas funcoes apreendidas na aula de\n",
        "# IA e machine learning do INFNET\n",
        "\n",
        "# passos para fazer ao analisar os dados\n",
        "#adicionar dependencias\n",
        "#carregar os dados dos dataset\n",
        "#analisar de dados\n",
        "#normalizar dados das colunas\n",
        "#verificar dados nullos\n",
        "#verificar dados categoricos\n",
        "#verificar representatividade dos dados do objetivo em relação ao restante dos dados\n",
        "#(objetivo = 1, restante = 0)\n",
        "#normalizar os dados\n",
        "#verificar separabilidade linear\n",
        "#separar os dados treino / teste estratificando\n",
        "#definir o baseline otimista / pessimista\n",
        "#gerar os dados estatisticos\n",
        "#otimizacao verificar os falsos positivos e falsos negativos em funcao do limiar minimo\n",
        "#otimizacao verificar o precisao e recall em funcao do limiar minimo\n",
        "#otimizacao verificar curva ROC vs classificador aleatorio\n",
        "#otimizacao lucro medio em relacao ao limiar minimo\n",
        "#gerar o modelo e os dados estatisticos\n",
        "#comparar com os modelos otimista e pessimistas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "import matplotlib.colors as colors\n",
        "# para normalizacao - MinMaxScaler\n",
        "# para padronizacao normal - StandardScaler\n",
        "# para padronizacao robusta - RobustScaler\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from google.colab import drive\n",
        "\n",
        "# ignorar warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9VFnVKeC52ys"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o codigo abaixo sao todas as funcoes utilizadas pelo algoritmo para tratar e analisar os dados, nao devem ser alteradas\n",
        "\n",
        "def loadAnaliseAndTransformData(cache, configuration, y_column):\n",
        "  data = loadDataSetFromGDriveOrCache(cache, configuration)\n",
        "\n",
        "  reAddDroppedColumnsBefore(configuration, cache, data)\n",
        "\n",
        "  if(configuration['normalize_columns']['enabled']):\n",
        "    print()\n",
        "    print('=> Normalizing column names...')\n",
        "    normalizeColumnNames(configuration, data)\n",
        "    print('=> Normalizing column names... OK')\n",
        "\n",
        "  if(configuration['drop_rows_by_value']['enabled']):\n",
        "    print()\n",
        "    print('=> Dropping rows by value...')\n",
        "    dropRowsByValue(configuration, data)\n",
        "    print('=> Dropping rows by value... OK')\n",
        "\n",
        "  transformData(configuration, cache, data)\n",
        "\n",
        "  if(configuration['drop_columns']['enabled']):\n",
        "    print()\n",
        "    print('=> Dropping columns...')\n",
        "    dropColumns(configuration, cache, data)\n",
        "    print('=> Dropping columns... OK')\n",
        "\n",
        "  if(configuration['drop_not_a_number_rows']['enabled']):\n",
        "    print()\n",
        "    print('=> Dropping not a number rows...')\n",
        "    data.dropna(inplace=True)\n",
        "    print('=> Dropping not a number rows... OK')\n",
        "\n",
        "  if(configuration['dataset']['show_data']['enabled']):\n",
        "    showDataInfo(configuration, data)\n",
        "\n",
        "  if(data.get(y_column) is None):\n",
        "    print('=> ERROR: Y column not found on dataset: ', y_column)\n",
        "    print('=> ERROR: ABORTING...OK')\n",
        "    return\n",
        "\n",
        "  if(configuration['pair_plot']['enabled']):\n",
        "    print()\n",
        "    print('=> Plotting pair plot...')\n",
        "    plotPairPlot(configuration, data)\n",
        "    print('=> Plotting pair plot... OK')\n",
        "\n",
        "  if(configuration['proportion_plot']['enabled']):\n",
        "    print()\n",
        "    print('=> Plotting proportions...')\n",
        "    proportionPlot(configuration, data)\n",
        "    print('=> Plotting proportions... OK')\n",
        "\n",
        "  if(configuration['prediction_models']['enabled']):\n",
        "    print()\n",
        "    print('=> Running models...')\n",
        "    runModels(configuration, data, y_column)\n",
        "    print('=> Running models... OK')\n",
        "\n",
        "  return data\n",
        "\n",
        "def dropRowsByValue(configuration, data):\n",
        "  for rule in configuration['drop_rows_by_value']['rules']:\n",
        "    if(not rule['enabled']):\n",
        "      continue\n",
        "    column = rule['column']\n",
        "    value_to_drop = rule['value_to_drop']\n",
        "    print()\n",
        "    print('=> Shape before dropping rows: ', data.shape)\n",
        "    print('=> Using column: ', column, ' and value: ', value_to_drop)\n",
        "    data.drop(data.index[(data[column] == value_to_drop)],axis=0,inplace=True)\n",
        "    print('=> Shape after dropping rows: ', data.shape)\n",
        "\n",
        "\n",
        "def loadDataSetFromGDriveOrCache(cache, configuration):\n",
        "  if(cache['dataset'] is None):\n",
        "    if(configuration['dataset']['source_provider'] == 'gdrive'):\n",
        "      print('=> Mounting google drive...')\n",
        "      drive.mount('/content/drive')\n",
        "      print('=> Mounting google drive... OK')\n",
        "    else:\n",
        "      print('=> ERROR: Source provider not supported: '+configuration['dataset']['source_provider'])\n",
        "      return\n",
        "\n",
        "    if(configuration['dataset']['reader_type'] == 'csv'):\n",
        "      print()\n",
        "      print('=> Reading csv data...')\n",
        "      cache['dataset'] = pd.read_csv('/content/drive/MyDrive/'+configuration['dataset']['path'], sep=configuration['dataset']['sep'])\n",
        "      print('=> Reading csv data... OK')\n",
        "      return cache['dataset']\n",
        "    else:\n",
        "      print('=> ERROR: Reader type not supported: '+configuration['dataset']['reader_type'])\n",
        "      return\n",
        "  else:\n",
        "    print()\n",
        "    print('=> Using cached dataset... OK')\n",
        "  return cache['dataset']\n",
        "\n",
        "def showDataInfo(configuration, data):\n",
        "  if(configuration['dataset']['show_data']['dataset_head']):\n",
        "    print()\n",
        "    print('=> Dataset head: ')\n",
        "    print(data.head())\n",
        "\n",
        "  if(configuration['dataset']['show_data']['dataset_info']):\n",
        "    print()\n",
        "    print('=> Dataset info: ')\n",
        "    print(data.info())\n",
        "\n",
        "  if(configuration['dataset']['show_data']['dataset_describe']):\n",
        "    print()\n",
        "    print('=> Dataset describe: ')\n",
        "    print(data.describe())\n",
        "\n",
        "  if(configuration['dataset']['show_data']['dataset_nulls']):\n",
        "    print()\n",
        "    print('=> Dataset nulls: ')\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "  if(configuration['dataset']['show_data']['dataset_duplicated']):\n",
        "    print()\n",
        "    print('=> Dataset duplicated: ')\n",
        "    print(data.duplicated().sum())\n",
        "\n",
        "  if(configuration['dataset']['show_data']['dataset_unique_values']):\n",
        "    print()\n",
        "    print('=> Dataset unique values: ')\n",
        "    for column in data.columns:\n",
        "      print()\n",
        "      print('=> Column: '+column, data[column].unique())\n",
        "\n",
        "  if(configuration['dataset']['show_data']['dataset_value_counts']):\n",
        "    print()\n",
        "    print('=> Dataset value counts: ')\n",
        "    for column in data.columns:\n",
        "      print()\n",
        "      print('=> Column: ', data[column].value_counts(normalize=True))\n",
        "  pass\n",
        "\n",
        "def transformData(configuration, cache, data):\n",
        "  for transformation in configuration['data_transformation']['transformations']:\n",
        "    columnName = transformation['column']\n",
        "    #backup original data column for cenarios where you need to run transformation multiple times\n",
        "    #and not want to loose original data\n",
        "    cachedColumns = cache['original_data_before_transformation']\n",
        "    if(not transformation['enabled']):\n",
        "      if(cachedColumns.get(columnName) is not None):\n",
        "        data[columnName] = cachedColumns[columnName]\n",
        "      continue\n",
        "    print()\n",
        "    print()\n",
        "    print('=> Applying data transformations...')\n",
        "    if(cachedColumns.get(columnName) is None):\n",
        "      cachedColumns[columnName] = data[columnName]\n",
        "    if(transformation['transform_to_new_column']['enabled']):\n",
        "      data[transformation['transform_to_new_column']['name']] = data[columnName].apply(transformation['function'])\n",
        "    else:\n",
        "      data[columnName] = cachedColumns[columnName].apply(transformation['function'])\n",
        "    print('=> Applying data transformations... OK')\n",
        "  pass\n",
        "\n",
        "def normalizeColumnNames(configuration, data):\n",
        "  normalize = configuration['normalize_columns']\n",
        "  if(normalize['modificator'] == 'lower'):\n",
        "    data.columns = data.columns.str.lower()\n",
        "  elif(normalize['modificator'] == 'upper'):\n",
        "    data.columns = data.columns.str.upper()\n",
        "  else:\n",
        "    print('=> ERROR: modificator unsuported!')\n",
        "  for column in data.columns:\n",
        "    for replacement in normalize['replacements']:\n",
        "      data.columns = data.columns.str.replace(replacement['char_to_replace'], replacement['replace_with'])\n",
        "  pass\n",
        "\n",
        "def reAddDroppedColumnsBefore(configuration, cache, data):\n",
        "  columns = []\n",
        "  for column in cache['original_data_before_drop']:\n",
        "    if(cache['original_data_before_drop'].get(column) is not None):\n",
        "      print()\n",
        "      print('=> Recriando coluna previamente deletada: ', column)\n",
        "      data[column] = cache['original_data_before_drop'][column]\n",
        "      columns.append(column)\n",
        "      print('=> Recriando coluna previamente deletada... OK')\n",
        "  for column in columns:\n",
        "     print()\n",
        "     print('=> Apagando coluna restaurada do cache: ', column)\n",
        "     del cache['original_data_before_drop'][column]\n",
        "     print('=> Apagando coluna restaurada do cache... OK')\n",
        "  pass\n",
        "\n",
        "def dropColumns(configuration, cache, data):\n",
        "  for column in configuration['drop_columns']['columns']:\n",
        "    cache['original_data_before_drop'][column] = data[column]\n",
        "  return data.drop(columns=configuration['drop_columns']['columns'], inplace=True)\n",
        "\n",
        "def checkColumnTypesAreNumbers(data, columns):\n",
        "  invalidColumns=[]\n",
        "  for column in columns:\n",
        "    for value in data[column].unique():\n",
        "      try:\n",
        "        float(value)\n",
        "      except ValueError:\n",
        "        print()\n",
        "        print('=> ERROR: this column cannot be used to pair plot because it not have numerical values: ', column)\n",
        "        print('=> ERROR: value: ', value)\n",
        "        invalidColumns.append(column)\n",
        "        break\n",
        "  for column in invalidColumns:\n",
        "    print()\n",
        "    print('=> Removing invalid column for pair plot: ', column)\n",
        "    columns.remove(column)\n",
        "  pass\n",
        "\n",
        "def plotPairPlot(configuration, data):\n",
        "  if(configuration['pair_plot']['type'] == '2d'):\n",
        "    hue = hue=configuration['pair_plot']['hue_column']\n",
        "    if(configuration['pair_plot']['columns'] == 'all'):\n",
        "      columns = []\n",
        "      for column in data.columns:\n",
        "        columns.append(column)\n",
        "      print()\n",
        "      print('=> Plotting pair plot with all columns: ', columns)\n",
        "      checkColumnTypesAreNumbers(data, columns)\n",
        "      sns.pairplot(data=data, hue=hue)\n",
        "    elif (configuration['pair_plot']['columns'] == 'selection'):\n",
        "      selection = configuration['pair_plot']['selection']\n",
        "      if(hue not in selection):\n",
        "        selection.append(hue)\n",
        "      checkColumnTypesAreNumbers(data, selection)\n",
        "      data_to_plot = data.loc[:,selection]\n",
        "      print()\n",
        "      print('=> Plotting pair plot with columns: ', selection)\n",
        "      sns.pairplot(data=data_to_plot, hue=hue)\n",
        "    else:\n",
        "      print('=> ERROR: Pair plot columns not supported: '+configuration['pair_plot']['columns'])\n",
        "      return\n",
        "  else:\n",
        "    print('=> ERROR: Pair plot type not supported: '+configuration['pair_plot']['type'])\n",
        "    return\n",
        "  pass\n",
        "\n",
        "def runModels(configuration, data, y_column):\n",
        "  unique_values = data[y_column].unique()\n",
        "  if(len(unique_values) == 2):\n",
        "    if((unique_values[0] == 1 or unique_values[0] == 0) and (unique_values[1] == 1 or unique_values[1] == 0)):\n",
        "\n",
        "      proportion = configuration['dataset']['proportion_of_test']\n",
        "      x = data.drop(columns=[y_column])\n",
        "      y = data[[y_column]]\n",
        "      x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=proportion, stratify=y)\n",
        "\n",
        "      for algoritm in configuration['prediction_models']['algoritms']:\n",
        "        if(not algoritm['enabled']):\n",
        "          continue\n",
        "\n",
        "        print()\n",
        "        print('=> Running model: ', algoritm['name'])\n",
        "\n",
        "        if(algoritm['type'] == 'predicted_by_0'):\n",
        "          y_predicted_0 = [0] * len(y_test)\n",
        "          if(algoritm['plot_graphic']):\n",
        "            print()\n",
        "            print('=> Imprimindo matriz de confusão (todas as respostas do modelo serao 0)')\n",
        "            printConfusionMatrix(y_test, y_predicted_0)\n",
        "          if(algoritm['print_metrics']):\n",
        "            print()\n",
        "            print('=> Imprimindo metricas do modelo (todas as respostas do modelo serao 0)')\n",
        "            print(classification_report(y_test, y_predicted_0))\n",
        "        elif(algoritm['type'] == 'predicted_by_1'):\n",
        "          y_predicted_1 = [1] * len(y_test)\n",
        "          if(algoritm['plot_graphic']):\n",
        "            print()\n",
        "            print('=> Imprimindo matriz de confusão (todas as respostas do modelo serao 1)')\n",
        "            printConfusionMatrix(y_test, y_predicted_1)\n",
        "          if(algoritm['print_metrics']):\n",
        "            print()\n",
        "            print('=> Imprimindo metricas do modelo (todas as respostas do modelo serao 1)')\n",
        "            print(classification_report(y_test, y_predicted_1))\n",
        "        elif(algoritm['type'] == 'pipeline'):\n",
        "          algoritm['pipeline'].fit(x_train, y_train)\n",
        "          y_predicted_test = algoritm['pipeline'].predict(x_test)\n",
        "          y_predicted_train = algoritm['pipeline'].predict(x_train)\n",
        "          if(algoritm['plot_graphic']):\n",
        "            print()\n",
        "            print('=> Imprimindo matriz de confusão (treino)')\n",
        "            printConfusionMatrix(y_train, y_predicted_train)\n",
        "            print()\n",
        "            print('=> Imprimindo matriz de confusão (teste)')\n",
        "            printConfusionMatrix(y_test, y_predicted_test)\n",
        "          if(algoritm['print_metrics']):\n",
        "            print()\n",
        "            print('=> Imprimindo metricas do modelo (treino)')\n",
        "            print(classification_report(y_train, y_predicted_train))\n",
        "            print()\n",
        "            print('=> Imprimindo metricas do modelo (teste)')\n",
        "            print(classification_report(y_test, y_predicted_test))\n",
        "\n",
        "    else:\n",
        "      print('=> ERROR: Y column has following values: ', unique_values)\n",
        "      print('=> ERROR: Y column must contain only two values: 0, 1')\n",
        "  else:\n",
        "    print('=> ERROR: Y column has following values: ', unique_values)\n",
        "    print('=> ERROR: Y column must contain only two values: 0, 1')\n",
        "  pass\n",
        "\n",
        "def printConfusionMatrix(data, y_optimist):\n",
        "  print()\n",
        "  cm = confusion_matrix(data[y_column], y_optimist)\n",
        "  labels = np.array([[\"TN[{}]\".format(cm[0,0]), \"FP[{}]\".format(cm[0,1])],\n",
        "                   [\"FN[{}]\".format(cm[1,0]), \"TP[{}]\".format(cm[1,1])]])\n",
        "  sns.heatmap(cm,  annot=labels, fmt='', cmap=\"Blues\")\n",
        "  plt.title('Matriz de Confusão - Regressão Logistica')\n",
        "  plt.xlabel('Saida do Modelo')\n",
        "  plt.ylabel('Saida Esperada')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  pass\n",
        "\n",
        "def proportionPlot(configuration, data):\n",
        "  if(configuration['proportion_plot']['columns'] == 'all'):\n",
        "    for column in data.columns:\n",
        "      proportionPlotGraphic(data, configuration, column)\n",
        "  elif (configuration['proportion_plot']['columns'] == 'selection'):\n",
        "    for selection in configuration['proportion_plot']['selection']:\n",
        "      if(selection['enabled']):\n",
        "        proportionPlotGraphic(data, configuration, selection['column'])\n",
        "  else:\n",
        "    print('=> ERROR: Pair plot columns not supported: '+configuration['pair_plot']['columns'])\n",
        "    return\n",
        "  pass\n",
        "\n",
        "def proportionPlotGraphic(data, configuration, column):\n",
        "  print()\n",
        "  print(\"=> Ploting proportion for column: \", column)\n",
        "  proportions = data[column].value_counts(normalize=True)\n",
        "  plt.bar(x=proportions.index, height=proportions.values)\n",
        "  plt.title('Proporção de valores da coluna: '+column)\n",
        "  plt.show()\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "J1U6IAkK58Od"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as variaveis abaixo mantem o cache de alguns dados antes das transformacoes,\n",
        "# drops ou carregamento do gdrive para melhorias de performance e integridade\n",
        "# das informacoes quando sao feitas execucoes de codigo parciais no colab\n",
        "cache = {\n",
        "    'dataset':None,\n",
        "    'original_data_before_transformation':{},\n",
        "    'original_data_before_drop':{},\n",
        "}"
      ],
      "metadata": {
        "id": "FTgEBTYE57TP"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta é a configuracao inicial nela voce irá alterar as variaveis para que\n",
        "# sua analise seja feita corretamente, nao se preocupe com a quantidade de\n",
        "# informacao e nem no que será colocado em cada campo neste momento\n",
        "# pois a maioria dos trechos estao desabilitados 'enabled': False entao nao\n",
        "# serao executados.\n",
        "\n",
        "# 1. Comece pela seção dataset, informando sua fonte de dados de acordo com os\n",
        "# comentarios.\n",
        "\n",
        "# 1.2. Na seção show_data será possível exibir os dados do dataset carregado.\n",
        "\n",
        "# Somente com essas duas seções preenchidas já será possível executar o codigo\n",
        "# e verificar os dados carregados no dataset.\n",
        "\n",
        "# 2. Na seção normalize_columns será possível transformar as colunas\n",
        "# substituindo caracteres.\n",
        "\n",
        "# 3. Na seção data_transformation será possível transformar os dados das linhas\n",
        "# atraves de uma função lambda que retornará o novo dado baseado na sua logica.\n",
        "\n",
        "# 4. Na seção drop_columns será possível excluir algumas colunas do dataset.\n",
        "\n",
        "# 5. Na seção drop_not_a_number_rows será possível excluir as linhas com valores\n",
        "# Not a Number do dataset.\n",
        "\n",
        "# 6. Na seção pair_plot será possível configurar a plotagem dos pares para\n",
        "# analise da separabilidade linear.\n",
        "# (Não esqueça de preencher a variavel 'y_column')\n",
        "\n",
        "# 7. Na seção prediction_models será possível configurar os algoritmos para\n",
        "# analise dos dados bem como suas propriedades.\n",
        "# (Não esqueça de preencher a variavel 'y_column')\n",
        "\n",
        "# 8. Na seção proportion_plot será possível fazer a plotagem de um grafico\n",
        "# para os valores de cada coluna do dataset.\n",
        "\n",
        "y_column = \"opinion\"\n",
        "\n",
        "configuration = {\n",
        "    \"show_debug_info\": True,  # TODO implementar nivel de log\n",
        "    \"dataset\": {\n",
        "        \"source_provider\": \"gdrive\",  # only supported google drive\n",
        "        \"reader_type\": \"csv\",  # only supported csv\n",
        "        \"path\": \"ALGORITMOS DE IA/DATASET/winequalityN.csv\",  # CAMINHO PARA O SEU DATASET NO GDRIVE\n",
        "        \"sep\": \",\",  # SEPARADOR DOS DADOS DO SEU DATASET, POR EX: , OU ; OU OUTRO\n",
        "        \"show_data\": {  # SECAO PARA CONFIGURACAO DOS LOGS\n",
        "            \"enabled\": True,\n",
        "            \"dataset_head\": False,\n",
        "            \"dataset_info\": False,\n",
        "            \"dataset_describe\": False,\n",
        "            \"dataset_nulls\": True,\n",
        "            \"dataset_duplicated\": True,\n",
        "            \"dataset_unique_values\": False,\n",
        "            \"dataset_value_counts\": False,\n",
        "        },\n",
        "        \"proportion_of_test\": 0.3,  # PERCENTUAL DO DATASET PARA SER USADO NO TREINO / TESTE\n",
        "    },\n",
        "    \"normalize_columns\": {  # SECAO PARA CONFIGURACAO DA NORMALIZACAO DAS COLUNAS\n",
        "        \"enabled\": True,\n",
        "        \"modificator\": \"lower\",  # ALLOWED ONLY 'lower' or 'upper' value\n",
        "        \"replacements\": [\n",
        "            {\"char_to_replace\": \" \", \"replace_with\": \"_\"},\n",
        "            {\"char_to_replace\": \"(\", \"replace_with\": \"\"},\n",
        "            {\"char_to_replace\": \")\", \"replace_with\": \"\"},\n",
        "        ],\n",
        "    },\n",
        "    \"data_transformation\": {  # SECAO PARA CONFIGURACAO DA TRANSFORMACAO DOS DADOS\n",
        "        \"transformations\": [\n",
        "            {\n",
        "                \"enabled\": True,\n",
        "                \"column\": \"quality\",\n",
        "                \"function\": lambda r: 1 if r > 5 else 0,\n",
        "                \"transform_to_new_column\":{\n",
        "                    \"enabled\": True,\n",
        "                    \"name\": \"opinion\"\n",
        "                }\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    \"drop_columns\": {\n",
        "        \"enabled\": True,\n",
        "        \"columns\": ['quality', 'type']\n",
        "    },\n",
        "    \"drop_not_a_number_rows\":{\n",
        "        \"enabled\":True,\n",
        "    },\n",
        "    \"drop_rows_by_value\":{\n",
        "        \"enabled\":True,\n",
        "        \"rules\":[\n",
        "            {\n",
        "                \"enabled\": True,\n",
        "                \"column\":\"type\",\n",
        "                \"value_to_drop\":\"red\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"pair_plot\": {\n",
        "        \"enabled\": False,\n",
        "        \"type\": \"2d\",  # only 2d available\n",
        "        \"columns\": \"all\",  # all or selection\n",
        "        \"selection\": [],\n",
        "        \"hue_column\": y_column,\n",
        "    },\n",
        "    \"prediction_models\": {\n",
        "        \"enabled\": True,\n",
        "        \"algoritms\": [\n",
        "            {\n",
        "                \"enabled\":True,\n",
        "                \"name\": \"Optimist Model\",\n",
        "                \"type\": \"predicted_by_0\",\n",
        "                \"plot_graphic\": False,\n",
        "                \"print_metrics\": True,\n",
        "                \"pipeline\": None,\n",
        "            },\n",
        "            {\n",
        "                \"enabled\":True,\n",
        "                \"name\": \"Pessimist Model\",\n",
        "                \"type\": \"predicted_by_1\",\n",
        "                \"plot_graphic\": False,\n",
        "                \"print_metrics\": True,\n",
        "                \"pipeline\": None,\n",
        "            },{\n",
        "                \"enabled\":True,\n",
        "                \"name\": \"Linear Regression\",\n",
        "                \"type\": \"pipeline\",\n",
        "                \"plot_graphic\": False,\n",
        "                \"print_metrics\": True,\n",
        "                \"pipeline\": Pipeline([\n",
        "                    ('scaler', RobustScaler()),\n",
        "                    ('model', LogisticRegression(penalty=None))\n",
        "                ]),\n",
        "            },\n",
        "\n",
        "                  ],\n",
        "    },\n",
        "    \"proportion_plot\":{\n",
        "        \"enabled\": False,\n",
        "        \"columns\": \"selection\",  # all or selection\n",
        "        \"selection\":[\n",
        "            {\n",
        "                \"enabled\": True,\n",
        "                \"column\": y_column,\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "}\n",
        "\n",
        "data = loadAnaliseAndTransformData(cache, configuration, y_column)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kojFcTeQKPwf",
        "outputId": "bb93b014-d69f-4fe2-af7d-25a3479339a3"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Mounting google drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "=> Mounting google drive... OK\n",
            "\n",
            "=> Reading csv data...\n",
            "=> Reading csv data... OK\n",
            "\n",
            "=> Normalizing column names...\n",
            "=> Normalizing column names... OK\n",
            "\n",
            "=> Dropping rows by value...\n",
            "\n",
            "=> Shape before dropping rows:  (6497, 13)\n",
            "=> Using column:  type  and value:  red\n",
            "=> Shape after dropping rows:  (4898, 13)\n",
            "=> Dropping rows by value... OK\n",
            "\n",
            "\n",
            "=> Applying data transformations...\n",
            "=> Applying data transformations... OK\n",
            "\n",
            "=> Dropping columns...\n",
            "=> Dropping columns... OK\n",
            "\n",
            "=> Dropping not a number rows...\n",
            "=> Dropping not a number rows... OK\n",
            "\n",
            "=> Dataset nulls: \n",
            "fixed_acidity           0\n",
            "volatile_acidity        0\n",
            "citric_acid             0\n",
            "residual_sugar          0\n",
            "chlorides               0\n",
            "free_sulfur_dioxide     0\n",
            "total_sulfur_dioxide    0\n",
            "density                 0\n",
            "ph                      0\n",
            "sulphates               0\n",
            "alcohol                 0\n",
            "opinion                 0\n",
            "dtype: int64\n",
            "\n",
            "=> Dataset duplicated: \n",
            "928\n",
            "\n",
            "=> Running models...\n",
            "\n",
            "=> Running model:  Optimist Model\n",
            "\n",
            "=> Imprimindo metricas do modelo (todas as respostas do modelo serao 0)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      1.00      0.50       489\n",
            "           1       0.00      0.00      0.00       972\n",
            "\n",
            "    accuracy                           0.33      1461\n",
            "   macro avg       0.17      0.50      0.25      1461\n",
            "weighted avg       0.11      0.33      0.17      1461\n",
            "\n",
            "\n",
            "=> Running model:  Pessimist Model\n",
            "\n",
            "=> Imprimindo metricas do modelo (todas as respostas do modelo serao 1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       489\n",
            "           1       0.67      1.00      0.80       972\n",
            "\n",
            "    accuracy                           0.67      1461\n",
            "   macro avg       0.33      0.50      0.40      1461\n",
            "weighted avg       0.44      0.67      0.53      1461\n",
            "\n",
            "\n",
            "=> Running model:  Linear Regression\n",
            "\n",
            "=> Imprimindo metricas do modelo (treino)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.52      0.59      1141\n",
            "           1       0.78      0.88      0.83      2268\n",
            "\n",
            "    accuracy                           0.76      3409\n",
            "   macro avg       0.73      0.70      0.71      3409\n",
            "weighted avg       0.75      0.76      0.75      3409\n",
            "\n",
            "\n",
            "=> Imprimindo metricas do modelo (teste)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.48      0.55       489\n",
            "           1       0.77      0.87      0.82       972\n",
            "\n",
            "    accuracy                           0.74      1461\n",
            "   macro avg       0.71      0.67      0.68      1461\n",
            "weighted avg       0.73      0.74      0.73      1461\n",
            "\n",
            "=> Running models... OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwyNel7i1hA_"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Olá, Alan,\n",
        "\n",
        "Nessa disciplina, aprendemos nossos conhecimentos em algoritmos supervisionados, família de algoritmos que é extremamente importante para o dia-a-dia de um cientista de dados. Agora iremos validar nosso conhecimento.\n",
        "\n",
        "Faça o módulo do Kaggle Intro to Machine Learning:\n",
        "Comprove a finalização do módulo com um print que contenha data e identificação do aluno.\n",
        "\n",
        "Trabalho com base:\n",
        "\n",
        "Iremos usar a base de dados de vinhos verdes portugueses (nas variantes branco e tinto) que encontra-se disponível no Kaggle:\n",
        "\n",
        "Para as questões 2-5 usaremos apenas os vinhos do tipo \"branco\".\n",
        "\n",
        "Faça o download da base - esta é uma base real, apresentada no artigo:\n",
        "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n",
        "\n",
        "Ela possui uma variável denominada \"quality\", uma nota de 0 a 10 que denota a qualidade do vinho. Crie uma nova variável, chamada \"opinion\" que será uma variável categórica igual à 0, quando quality for menor e igual à 5. O valor será 1, caso contrário. Desconsidere a variável quality para o restante da análise.\n",
        "\n",
        "Descreva as variáveis presentes na base. Quais são as variáveis? Quais são os tipos de variáveis (discreta, categórica, contínua)? Quais são as médias e desvios padrões?\n",
        "\n",
        "Com a base escolhida:\n",
        "\n",
        "Descreva as etapas necessárias para criar um modelo de classificação eficiente.\n",
        "\n",
        "Treine um modelo de regressão logística usando um modelo de validação cruzada estratificada com k-folds (k=10) para realizar a classificação. Calcule para a base de teste:\n",
        "i. a média e desvio da acurácia dos modelos obtidos;\n",
        "ii. a média e desvio da precisão dos modelos obtidos;\n",
        "iii. a média e desvio da recall dos modelos obtidos;\n",
        "iv. a média e desvio do f1-score dos modelos obtidos.\n",
        "\n",
        "Treine um modelo de árvores de decisão usando um modelo de validação cruzada estratificada com k-folds (k=10) para realizar a classificação. Calcule para a base de teste:\n",
        "i. a média e desvio da acurácia dos modelos obtidos;\n",
        "ii. a média e desvio da precisão dos modelos obtidos;\n",
        "iii. a média e desvio da recall dos modelos obtidos;\n",
        "iv. a média e desvio do f1-score dos modelos obtidos.\n",
        "\n",
        "Treine um modelo de SVM usando um modelo de validação cruzada estratificada com k-folds (k=10) para realizar a classificação. Calcule para a base de teste:\n",
        "i. a média e desvio da acurácia dos modelos obtidos;\n",
        "ii. a média e desvio da precisão dos modelos obtidos;\n",
        "iii. a média e desvio da recall dos modelos obtidos;\n",
        "iv. a média e desvio do f1-score dos modelos obtidos.\n",
        "\n",
        "Em relação à questão anterior, qual o modelo deveria ser escolhido para uma eventual operação. Responda essa questão mostrando a comparação de todos os modelos, usando um gráfico mostrando a curva ROC média para cada um dos gráficos e justifique.\n",
        "\n",
        "Com a escolha do melhor modelo, use os dados de vinho tinto, presentes na base original e faça a inferência (não é para treinar novamente!!!) para saber quantos vinhos são bons ou ruins. Utilize o mesmo critério utilizado com os vinhos brancos, para comparar o desempenho do modelo. Ele funciona da mesma forma para essa nova base? Justifique.\n",
        "\n",
        "Disponibilize os códigos usados para responder da questão 2-6 em uma conta github e indique o link para o repositório.\n",
        "Assim que terminar, salve o seu arquivo PDF e poste no Moodle. Utilize o seu nome para nomear o arquivo, identificando também a disciplina no seguinte formato: “nomedoaluno_nomedadisciplina_pd.PDF”."
      ],
      "metadata": {
        "id": "0sHxrMK11h4A"
      }
    }
  ]
}