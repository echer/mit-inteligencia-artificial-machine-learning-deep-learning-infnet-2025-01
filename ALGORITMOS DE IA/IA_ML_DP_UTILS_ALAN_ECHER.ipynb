{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbpngF0a5twG"
      },
      "outputs": [],
      "source": [
        "#passos para fazer ao analisar os dados\n",
        "#adicionar dependencias\n",
        "#carregar os dados dos dataset\n",
        "#analisar de dados\n",
        "#normalizar dados das colunas\n",
        "#verificar dados nullos\n",
        "#verificar dados categoricos\n",
        "#verificar representatividade dos dados do objetivo em relação ao restante dos dados\n",
        "#(objetivo = 1, restante = 0)\n",
        "#normalizar os dados\n",
        "#verificar separabilidade linear\n",
        "#separar os dados treino / teste estratificando\n",
        "#definir o baseline otimista / pessimista\n",
        "#gerar os dados estatisticos\n",
        "#otimizacao verificar os falsos positivos e falsos negativos em funcao do limiar minimo\n",
        "#otimizacao verificar o precisao e recall em funcao do limiar minimo\n",
        "#otimizacao verificar curva ROC vs classificador aleatorio\n",
        "#otimizacao lucro medio em relacao ao limiar minimo\n",
        "#gerar o modelo e os dados estatisticos\n",
        "#comparar com os modelos otimista e pessimistas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "9VFnVKeC52ys"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# esta variavel mantem o cache do dataset que foi carregado do gdrive\n",
        "# para evitar carregar toda vez os dados\n",
        "# caso precise carregar novamente os dados só rode este bloco e o cache será limpo\n",
        "cache = {\n",
        "    'dataset':None,\n",
        "    'original_data_before_transformation':{}\n",
        "}"
      ],
      "metadata": {
        "id": "-0wjnn-VKcvm"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadAnaliseAndTransformData(cache, configuration):\n",
        "  data = loadDataSetFromGDriveOrCache(cache, configuration)\n",
        "\n",
        "  if(configuration['dataset']['normalizeColumns']['enabled']):\n",
        "    print()\n",
        "    print('=> Normalizing column names...')\n",
        "    normalizeColumnNames(configuration, data)\n",
        "    print('=> Normalizing column names... OK')\n",
        "\n",
        "  if(configuration['data_transformation']['enabled']):\n",
        "    print()\n",
        "    print('=> Applying data transformations...')\n",
        "    transformData(configuration, cache, data)\n",
        "    print('=> Applying data transformations... OK')\n",
        "\n",
        "  showDataInfo(configuration, data)\n",
        "\n",
        "  return data\n",
        "\n",
        "def loadDataSetFromGDriveOrCache(cache, configuration):\n",
        "  if(cache['dataset'] is None):\n",
        "    print('=> Mounting drive...')\n",
        "    drive.mount('/content/drive')\n",
        "    print('=> Mounting drive... OK')\n",
        "\n",
        "    print()\n",
        "    print('=> Loading dataset...')\n",
        "    data = pd.read_csv('/content/drive/MyDrive/'+configuration['dataset']['path'], sep=configuration['dataset']['sep'])\n",
        "    cache['dataset'] = data\n",
        "    print('=> Loading dataset... OK')\n",
        "  else:\n",
        "    print()\n",
        "    print('=> Using cached dataset... OK')\n",
        "  return cache['dataset']\n",
        "\n",
        "def showDataInfo(configuration, data):\n",
        "  if(configuration['dataset']['show_log']['dataset_head']):\n",
        "    print()\n",
        "    print('=> Dataset head: ')\n",
        "    print(data.head())\n",
        "\n",
        "  if(configuration['dataset']['show_log']['dataset_info']):\n",
        "    print()\n",
        "    print('=> Dataset info: ')\n",
        "    print(data.info())\n",
        "\n",
        "  if(configuration['dataset']['show_log']['dataset_describe']):\n",
        "    print()\n",
        "    print('=> Dataset describe: ')\n",
        "    print(data.describe())\n",
        "\n",
        "  if(configuration['dataset']['show_log']['dataset_nulls']):\n",
        "    print()\n",
        "    print('=> Dataset nulls: ')\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "  if(configuration['dataset']['show_log']['dataset_duplicated']):\n",
        "    print()\n",
        "    print('=> Dataset duplicated: ')\n",
        "    print(data.duplicated().sum())\n",
        "\n",
        "  if(configuration['dataset']['show_log']['dataset_unique_values']):\n",
        "    print()\n",
        "    print('=> Dataset unique values: ')\n",
        "    for column in data.columns:\n",
        "      print('Column: '+column, data[column].unique())\n",
        "\n",
        "  if(configuration['dataset']['show_log']['dataset_value_counts']):\n",
        "    print()\n",
        "    print('=> Dataset value counts: ')\n",
        "    for column in data.columns:\n",
        "      print('Column: '+column, data[column].value_counts())\n",
        "  pass\n",
        "\n",
        "def transformData(configuration, cache, data):\n",
        "  for transformation in configuration['data_transformation']['transformations']:\n",
        "    columnName = transformation['column']\n",
        "    #backup original data column for cenarios where you need to run transformation multiple times\n",
        "    #and not want to loose original data\n",
        "    cachedColumns = cache['original_data_before_transformation']\n",
        "    if(cachedColumns.get(columnName) is None):\n",
        "      cachedColumns[columnName] = data[columnName]\n",
        "    data[columnName] = cachedColumns[columnName].apply(transformation['function'])\n",
        "  pass\n",
        "\n",
        "def normalizeColumnNames(configuration, data):\n",
        "  normalize = configuration['dataset']['normalizeColumns']\n",
        "  if(normalize['modificator'] == 'lower'):\n",
        "    data.columns = data.columns.str.lower()\n",
        "  elif(normalize['modificator'] == 'upper'):\n",
        "    data.columns = data.columns.str.upper()\n",
        "  for column in data.columns:\n",
        "    for replacement in normalize['replacements']:\n",
        "      data[column] = data[column].replace(replacement['charToReplace'], replacement['replaceWith'])\n",
        "  pass\n",
        "\n"
      ],
      "metadata": {
        "id": "J1U6IAkK58Od"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#configuracao inicial\n",
        "configuration = {\n",
        "    'dataset':{\n",
        "        'path':'ALGORITMOS DE IA/DATASET/drug200.csv',# CAMINHO PARA O SEU DATASET NO GDRIVE\n",
        "        'sep':',',# SEPARADOR DOS DADOS DO SEU DATASET, POR EX: , OU ; OU OUTRO\n",
        "        'normalizeColumns':{# SECAO PARA CONFIGURACAO DA NORMALIZACAO DAS COLUNAS\n",
        "            'enabled':True,\n",
        "            'modificator': 'lower',# ALLOWED ONLY 'lower' or 'upper' value\n",
        "            'replacements':[\n",
        "                {\n",
        "                    'charToReplace':' ',\n",
        "                    'replaceWith':'_'\n",
        "                },\n",
        "                {\n",
        "                    'charToReplace':'(',\n",
        "                    'replaceWith':''\n",
        "                },\n",
        "                {\n",
        "                    'charToReplace':')',\n",
        "                    'replaceWith':''\n",
        "                },\n",
        "            ]\n",
        "        },\n",
        "        'show_log':{# SECAO PARA CONFIGURACAO DOS LOGS\n",
        "            'dataset_head':True,\n",
        "            'dataset_info':False,\n",
        "            'dataset_describe':False,\n",
        "            'dataset_nulls':False,\n",
        "            'dataset_duplicated':False,\n",
        "            'dataset_unique_values':False,\n",
        "            'dataset_value_counts':False,\n",
        "        },\n",
        "    },\n",
        "    'data_transformation':{ #SECAO PARA CONFIGURACAO DA TRANSFORMACAO DOS DADOS\n",
        "        'enabled':True,\n",
        "        'transformations':[\n",
        "            {\n",
        "                'column':'sex',\n",
        "                'function':lambda r:1 if r == 'M' else 0\n",
        "            },\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "data = loadAnaliseAndTransformData(cache, configuration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kojFcTeQKPwf",
        "outputId": "0d953356-6d79-4a40-ec14-20017ef6480a"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Using cached dataset... OK\n",
            "\n",
            "=> Normalizing column names...\n",
            "=> Normalizing column names... OK\n",
            "\n",
            "=> Applying data transformations...\n",
            "=> Applying data transformations... OK\n",
            "\n",
            "=> Dataset head: \n",
            "   age  sex      bp cholesterol  na_to_k   drug\n",
            "0   23    0    HIGH        HIGH   25.355  drugY\n",
            "1   47    1     LOW        HIGH   13.093  drugC\n",
            "2   47    1     LOW        HIGH   10.114  drugC\n",
            "3   28    0  NORMAL        HIGH    7.798  drugX\n",
            "4   61    0     LOW        HIGH   18.043  drugY\n"
          ]
        }
      ]
    }
  ]
}